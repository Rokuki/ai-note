## mobilenet为什么快
1. 将标准化卷积改为深度可分离卷积（Depthwise Separable Convolution）
深度可分离卷积包含以下两种
- 逐深度卷积（depthwise convolution）(逐层卷积)
- 逐点1x1卷积（pointwise convolution）(逐点卷积)
![标准卷积分解过程](https://pic3.zhimg.com/80/v2-a6bae41f1744363354d5b931bc6a9f06_1440w.jpg)

### 常规卷积运算
假设输入层为一个大小为64×64像素、三通道彩色图片。经过一个包含4个Filter的卷积层，最终输出4个Feature Map，且尺寸与输入层相同。整个过程可以用下图来概括。
![dd](https://github.com/Rokuki/ai-note/blob/main/interview/img/7g19p3crf0.jpeg)

此时，卷积层共4个Filter，每个Filter包含了3个Kernel，每个Kernel的大小为3×3。因此卷积层的参数数量可以用如下公式来计算：

N_std = 4 × 3 × 3 × 3 = 108

### Separable Convolution

Separable Convolution在Google的Xception[1]以及MobileNet[2]论文中均有描述。它的核心思想是将一个完整的卷积运算分解为两步进行，分别为Depthwise Convolution与Pointwise Convolution。

### Depthwise Convolution
同样是上述例子，一个大小为64×64像素、三通道彩色图片首先经过第一次卷积运算，不同之处在于此次的卷积完全是在二维平面内进行，且Filter的数量与上一层的Depth相同。所以一个三通道的图像经过运算后生成了3个Feature map，如下图所示。
![ww](https://github.com/Rokuki/ai-note/blob/main/interview/img/v8k0v6b8ah.jpeg?raw=true)

其中一个Filter只包含一个大小为3×3的Kernel，卷积部分的参数个数计算如下：

- N_depthwise = 3 × 3 × 3 = 27

Depthwise Convolution完成后的Feature map数量与输入层的depth相同，但是这种运算对输入层的每个channel独立进行卷积运算后就结束了，没有有效的利用不同map在相同空间位置上的信息。因此需要增加另外一步操作来将这些map进行组合生成新的Feature map，即接下来的Pointwise Convolution。

### Pointwise Convolution

Pointwise Convolution的运算与常规卷积运算非常相似，不同之处在于卷积核的尺寸为 1×1×M，M为上一层的depth。所以这里的卷积运算会将上一步的map在深度方向上进行加权组合，生成新的Feature map。有几个Filter就有几个Feature map。如下图所示。
![ww](https://github.com/Rokuki/ai-note/blob/main/interview/img/ugcnzd39tq.jpeg?raw=true)

由于采用的是1×1卷积的方式，此步中卷积涉及到的参数个数可以计算为：

- N_pointwise = 1 × 1 × 3 × 4 = 12

经过Pointwise Convolution之后，同样输出了4张Feature map，与常规卷积的输出维度相同。

参数对比
回顾一下，常规卷积的参数个数为：

- N_std = 4 × 3 × 3 × 3 = 108

Separable Convolution的参数由两部分相加得到：

- N_depthwise = 3 × 3 × 3 = 27
- N_pointwise = 1 × 1 × 3 × 4 = 12
- N_separable = N_depthwise + N_pointwise = 39

相同的输入，同样是得到4张Feature map，Separable Convolution的参数个数是常规卷积的约1/3。因此，在参数量相同的前提下，采用Separable Convolution的神经网络层数可以做的更深。

2. 关于速度

> 在算力足够的GPU平台上，MobileNet不会带来任何速度上的提升（有时甚至是下降的），然而在计算能力有限的平台上，MobileNet能让速度提升三倍以上。

深度可分离卷积将一个标准卷积分割成了两个卷积（逐深度，逐点），因此减小了参数量，对应也减小了总计算量。

深度可分离卷积的总计算量变小了，但深度可分离卷积的层数变多了。

GPU是并行处理大规模数据（矩阵内积）的运算平台，而CPU则倾向于对数据串行计算（一个一个算）。

因此，若GPU的显存足够大（干脆假设无限大好了），因为每层的计算都可以并行一次处理，则此时总运算时间的主导因素是网络的层数。



## 如何通过参数查看cpu、gpu等性能
cpu:
1. 频率
2. 制程
3. 缓存
4. 功耗
5. 核心架构

gpu：
1. 显卡架构
2. 位宽
3. 流处理器数量
4. 核芯频率
5. 显存
6. ROPs
7. CUDA 核心数量

## onnxruntime插件、cuda

支持cuda


## cuda编程
1. 分配host内存，并进行数据初始化；
2. 分配device内存，并从host将数据拷贝到device上；
3. 调用CUDA的核函数在device上完成指定的运算；
4. 将device上的运算结果拷贝到host上；
5. 释放device和host上分配的内存。

## torch.funtional.nn 和torch.nn的区别
torch.nn是包装好的类，torch.funtional.nn是可以直接调用的函数

nn.Xxx 需要先实例化并传入参数，然后以函数调用的方式调用实例化的对象并传入输入数据。
```python
inputs = torch.rand(64, 3, 244, 244)
conv = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)
out = conv(inputs)
```

nn.functional.xxx同时传入输入数据和weight, bias等其他参数。
```python
weight = torch.rand(64,3,3,3)
bias = torch.rand(64) 
out = nn.functional.conv2d(inputs, weight, bias, padding=1)
```
- nn.Xxx继承于nn.Module， 能够很好的与nn.Sequential结合使用， 而nn.functional.xxx无法与nn.Sequential结合使用。

- nn.Xxx不需要你自己定义和管理weight；而nn.functional.xxx需要你自己定义weight，每次调用的时候都需要手动传入weight, 不利于代码复用。

torch.nn下的Conv1d:

```python
class Conv1d(_ConvNd):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1,
                 padding=0, dilation=1, groups=1, bias=True):
        kernel_size = _single(kernel_size)
        stride = _single(stride)
        padding = _single(padding)
        dilation = _single(dilation)
        super(Conv1d, self).__init__(
            in_channels, out_channels, kernel_size, stride, padding, dilation,
            False, _single(0), groups, bias)

    def forward(self, input):
        return F.conv1d(input, self.weight, self.bias, self.stride,
                        self.padding, self.dilation, self.groups)
```
torch.nn.functional下的conv1d:

```python
def conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1,
           groups=1):
    if input is not None and input.dim() != 3:
        raise ValueError("Expected 3D tensor as input, got {}D tensor instead.".format(input.dim()))

    f = ConvNd(_single(stride), _single(padding), _single(dilation), False,
               _single(0), groups, torch.backends.cudnn.benchmark,
               torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)
    return f(input, weight, bias)

```
