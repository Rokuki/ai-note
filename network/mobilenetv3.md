## h-swish
- 仅用于深层次网络（特征图在较浅的层中往往更大，因此计算其激活成本更高，所以采用ReLU）

![dd](https://img-blog.csdnimg.cn/20191209213051844.png)

![dd](https://github.com/Rokuki/ai-note/blob/main/network/img/h_swish_latency.jpg)

h-swish速度比ReLU慢，比swish快；精度较ReLU有显著提高

