## h-swish
- 仅用于深层次网络（特征图在较浅的层中往往更大，因此计算其激活成本更高，所以采用ReLU）

![dd](https://img-blog.csdnimg.cn/20191209213051844.png)

![dd](https://github.com/Rokuki/ai-note/blob/main/network/img/h_swish_latency.jpg)
h-swish相比ReLU，增加了延时，显著提高了精度
